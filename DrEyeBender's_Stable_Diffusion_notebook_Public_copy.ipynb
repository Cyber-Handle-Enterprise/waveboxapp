{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DrEyeBender's Stable Diffusion notebook - Public copy.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Cyber-Handle-Enterprise/waveboxapp/blob/master/DrEyeBender's_Stable_Diffusion_notebook_Public_copy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DrEyeBender's Stable Diffusion notebook\n",
        "\n",
        "Demo videos:\n",
        "\n",
        "Musicians and Game of Thrones characters\n",
        "https://www.youtube.com/watch?v=4MQVlVZO9JU&t=2s\n",
        "\n",
        "Dogs turning into cats tunring into birds tunring back into dogs\n",
        "https://www.youtube.com/watch?v=vl1vij9tpyI&t=1s\n",
        "\n",
        "Fork it: https://github.com/isaac-bender/stable_diffusion_interp/blob/main/DrEyeBender's_Stable_Diffusion_notebook_Public_copy.ipynb\n",
        "\n",
        "This is based on https://github.com/CompVis/stable-diffusion/blob/main/scripts/txt2img.py\n",
        "\n",
        "Stuff I added:\n",
        "1.   Prompt interpolation\n",
        "2.   Multiple prompts per batch\n",
        "3.   Better output naming\n",
        "4.   Automatic grid layout\n",
        "5.   Prompt prefixes\n",
        "6.   Random start code options (shared or different per sample in batch)\n",
        "7.   Fixed or new start code per batch\n",
        "\n",
        "TODOs:\n",
        "1.   ~Installation~\n",
        "2.   Assemble a video out of the rendered frames\n",
        "3.   Init images\n",
        "4.   Probably some debugging :)\n",
        "5.   Bring back prompt file support (temporarily broken)\n",
        "6.   Add more samplers\n",
        "7.   Prompt combiner\n",
        "8.   Menus for enumerated choices (rather than string entry)\n",
        "9.   Cleanup of some copypasta\n",
        "10.  Reorganize the settings so they're grouped more logically\n",
        "\n",
        "# Use examples\n",
        "These are just a few suggestions that explain how you can use the options. Of course, these aren't the only valid configurations. As you become familiar with the options, I'm sure you'll think of some more creative ways to use them.\n",
        "\n",
        "Example use case: Prompt variation test\n",
        "---\n",
        "You want to compare the effetcs of variations on a prompt in a controlled way\n",
        "*   use_prefixes = True\n",
        "*   prompt_mode = 'per_batch'\n",
        "*   new_start_code_per_sample = False\n",
        "*   seed = [whatever number you want]\n",
        "\n",
        "Put your prompts in the prompt list, and put your prefixes in the prefix list.\n",
        "\n",
        "Each sample gets the same prompt, but with a different prefix.\n",
        "\n",
        "You could also not use prefixes, and set prompt mode to 'per_sample'. That will put a different propmpt in each cell.\n",
        "\n",
        "They will all be seeded the exact same way, so you can compare the effects of the prefixes or different prompts in isolation.\n",
        "\n",
        "Example use case: Just render a bunch of prompts\n",
        "---\n",
        "*   use_prefixes = False\n",
        "*   prompt_mode = 'per_sample'\n",
        "*   new_start_code_per_sample = True\n",
        "*   new_start_code_per_batch = True\n",
        "*   seed = int(time.time())\n",
        "\n",
        "Put your prompts in the prompt list.\n",
        "\n",
        "Each sample in the batch will have a different propmpt and seed, and each batch will get a new set of seeds. This way, you get maximum variety.\n",
        "\n",
        "Example use case: Prompt interpolation\n",
        "---\n",
        "Now it's starting to get interesting :)\n",
        "*   num_interpolation_steps = 10 #higher numbers will give smoother animations\n",
        "*   use_prefixes = [up to you]\n",
        "*   prompt_mode = 'per_sample'\n",
        "*   new_start_code_per_sample = [up to you]\n",
        "*   num_interpolation_steps = 10 #higher numbers will give smoother animations\n",
        "\n",
        "Put your prompts in the prompt list.\n",
        "\n",
        "Each sample in the batch will have a different propmpt.\n",
        "\n",
        "**If new_start_code_per_batch is True, the start codes will also be interpolated from batch to batch.**\n",
        "\n",
        "The learned conditioning for each prompt is interpolated between itself and the learned conditioning of the prompt in the same position in the next batch.\n",
        "\n",
        "num_interpolation_steps controls how smooth the animation is. 10 is just an example starting point. If you have all night, set it to 1000, I'm sure that would look neat!\n",
        "\n",
        "At the end of the prompt list, it wraps around to interpolate the last batch into the first.\n",
        "\n",
        "It may look like it doesn't render the very last frame, but that frame would be identical to the first frame anyway.\n",
        "\n",
        "---\n",
        "p.s., Share your prompts! Concealing your prompts is a coward's move."
      ],
      "metadata": {
        "id": "cKzmLtBDqTjH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clone repo and install packages\n",
        "\n",
        "**'do_install' defaults False in order to help prevent redundant / unintentional insatlls**\n",
        "\n",
        "This is from https://colab.research.google.com/github/cpacker/stable-diffusion/blob/interactive-notebook/scripts/stable_diffusion_interactive_colab.ipynb"
      ],
      "metadata": {
        "id": "JgY7H-pZ_Zib"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "  # Check what GPU we're using\n",
        "  !nvidia-smi"
      ],
      "metadata": {
        "id": "NSGV6mB8pjbt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#defaults False in order to help prevent redundant / unintentional insatlls\n",
        "do_install = False #@param {type:\"boolean\"}\n",
        "get_weights_from_gdrive = True #@param {type:\"boolean\"}\n",
        "if do_install:\n",
        "  #@title Run once { display-mode: \"form\" }\n",
        "\n",
        "  # clone repo\n",
        "  !git clone https://github.com/CompVis/stable-diffusion.git\n",
        "\n",
        "  # base colab installs cause issues in lightning.seed_everything\n",
        "  !pip uninstall -y torchtext\n",
        "\n",
        "  # Copy-pasta of https://github.com/cpacker/stable-diffusion/blob/main/environment.yaml\n",
        "  # But try skipping the torch and cudatoolkit installs\n",
        "  #!pip install numpy==1.19.2  # omit, causes this issue: https://stackoverflow.com/questions/66060487/valueerror-numpy-ndarray-size-changed-may-indicate-binary-incompatibility-exp\n",
        "  !pip install albumentations==0.4.3\n",
        "  !pip install opencv-python==4.1.2.30\n",
        "  !pip install pudb==2019.2\n",
        "  !pip install imageio==2.9.0\n",
        "  !pip install imageio-ffmpeg==0.4.2\n",
        "  !pip install pytorch-lightning==1.4.2\n",
        "  !pip install omegaconf==2.1.1\n",
        "  !pip install test-tube>=0.7.5\n",
        "  !pip install streamlit>=0.73.1\n",
        "  !pip install einops==0.3.0\n",
        "  !pip install torch-fidelity==0.3.0\n",
        "  !pip install transformers==4.19.2\n",
        "  !pip install torchmetrics==0.6.0\n",
        "  !pip install kornia==0.6\n",
        "  !pip install git+https://github.com/crowsonkb/k-diffusion\n",
        "  !pip install -e git+https://github.com/CompVis/taming-transformers.git@master#egg=taming-transformers\n",
        "  !pip install -e git+https://github.com/openai/CLIP.git@main#egg=clip\n",
        "  !pip install -e .\n",
        "\n",
        "  # Colab broke widget support on 8/19/2022, here's the temp fix:\n",
        "  # https://github.com/googlecolab/colabtools/issues/3020\n",
        "  !pip install \"ipywidgets>=7,<8\"  \n",
        "%cd /content/stable-diffusion\n"
      ],
      "metadata": {
        "id": "8iHI6LGc_gbd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Linking your copy of the Stable Diffusion weights\n",
        "*This notebook assumes you already have access to the Stable Diffusion weights.*"
      ],
      "metadata": {
        "id": "1hlqz1NaOWc1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Upload the weights to Google Drive, then mount into Colab"
      ],
      "metadata": {
        "id": "ca-ptaqpFviV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Upload your copy of the weights (e.g., `sd-v1-4.ckpt`) to a folder on your Drive called \"stable-diffusion-checkpoints\" (or change the following code to match the path where the weights are stored on your account). If you didn't put your weights in a Drive folder called `stable-diffusion-checkpoints`, update `/content/drive/stable-diffusion-checkpoints/sd-v1-4.ckpt` accordingly.\n",
        "\n",
        "You can also mount the Drive folder using the Colab file browser."
      ],
      "metadata": {
        "id": "_ZsHoEfkJJq3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_path_from_gdrive = None\n",
        "# This will open a pop-up window asking you to link your Google Drive account to this notebook for access\n",
        "if get_weights_from_gdrive:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive/')\n",
        "  model_path_from_gdrive = '/content/drive/MyDrive/stable-diffusion-checkpoints/sd-v1-4.ckpt'  "
      ],
      "metadata": {
        "id": "yx3s4k3LO0zX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Init"
      ],
      "metadata": {
        "id": "_Z5uA1j8_Wmr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython import display\n",
        "import argparse, os, sys, glob\n",
        "import torch\n",
        "torch.backends.cudnn.benchmark = False\n",
        "import numpy as np\n",
        "from omegaconf import OmegaConf\n",
        "from PIL import Image\n",
        "from tqdm.auto import tqdm, trange\n",
        "from itertools import islice\n",
        "from einops import rearrange\n",
        "from torchvision.utils import make_grid\n",
        "import time\n",
        "from pytorch_lightning import seed_everything\n",
        "from torch import autocast\n",
        "from contextlib import contextmanager, nullcontext\n",
        "\n",
        "try:\n",
        "  # if you have a local installation, you can cd to it here\n",
        "  # either set the env var, or just set the path here directly\n",
        "  sd_path = os.getenv(\"LOCAL_STABLE_DIFFUSION_PATH\")\n",
        "  os.chdir(sd_path)\n",
        "  sys.path.append(os.getcwd())\n",
        "except Exception as e:\n",
        "  pass\n",
        "\n",
        "from ldm.util import instantiate_from_config\n",
        "from ldm.models.diffusion.ddim import DDIMSampler\n",
        "from ldm.models.diffusion.plms import PLMSSampler"
      ],
      "metadata": {
        "id": "Pc_LO3VOrw7u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utility functions"
      ],
      "metadata": {
        "id": "W20ujOogA6UH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image, ImageFont, ImageDraw\n",
        "import math\n",
        "import numpy as np\n",
        "import time\n",
        "import io\n",
        "import k_diffusion as K\n",
        "import torch.nn as nn\n",
        "\n",
        "def load_model_from_config(config, ckpt, verbose=False):\n",
        "    print(f\"Loading model from {ckpt}\")\n",
        "    pl_sd = torch.load(ckpt, map_location=\"cpu\")\n",
        "    if \"global_step\" in pl_sd:\n",
        "        print(f\"Global Step: {pl_sd['global_step']}\")\n",
        "    sd = pl_sd[\"state_dict\"]\n",
        "    model = instantiate_from_config(config.model)\n",
        "    m, u = model.load_state_dict(sd, strict=False)\n",
        "    if len(m) > 0 and verbose:\n",
        "        print(\"missing keys:\")\n",
        "        print(m)\n",
        "    if len(u) > 0 and verbose:\n",
        "        print(\"unexpected keys:\")\n",
        "        print(u)\n",
        "\n",
        "    model.cuda()\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "def clean_filename(str):\n",
        "  #print(str)\n",
        "  try:\n",
        "    while str[0] == ' ':\n",
        "      str = str[1:]\n",
        "    str = ''.join([ c if c.isalnum() else ' ' for c in str ])\n",
        "    str = str.replace('  ', ' ')\n",
        "    str = str.replace(' ', '-')\n",
        "  except:\n",
        "    pass\n",
        "  return str\n",
        "\n",
        "def make_start_code(batch_size, shared_code, C, H, W, f, device):\n",
        "  if shared_code:\n",
        "    #the same random start code is shared by each sample\n",
        "    start_code_max = torch.randn([C, H // f, W // f], device=device)\n",
        "    start_code_max = start_code_max.repeat(batch_size, *[1 for n in range(len(start_code_max.shape))])\n",
        "  else:\n",
        "    #start code is random for the whole tensor, so each sample is different\n",
        "    start_code_max = torch.randn([batch_size, C, H // f, W // f], device=device)    \n",
        "\n",
        "  return start_code_max\n",
        "\n",
        "try:\n",
        "  from matplotlib import font_manager\n",
        "  system_fonts = font_manager.findSystemFonts(fontpaths=None, fontext='ttf')\n",
        "  #print('\\n'.join(system_fonts))\n",
        "  for font_name in system_fonts:\n",
        "    if 'segoeui.ttf' in font_name.lower() or 'liberationsans-regular.ttf' in font_name.lower():\n",
        "      font = ImageFont.truetype(font_name, 14)\n",
        "      break\n",
        "except:\n",
        "  font = ImageFont.load_default()\n",
        "\n",
        "def draw_text(text, x, y, drawing_context, font):\n",
        "\tdrawing_context.text((x+2, y+2), text, font = font, fill=(0, 0, 0))\n",
        "\tdrawing_context.text((x, y), text, font = font, fill=(255, 255, 255))\n",
        "\n",
        "def get_grid_dims(num_elements):\n",
        "  best_diff = num_elements+1\n",
        "  for x in range(1, num_elements+1):\n",
        "    y = num_elements // x\n",
        "    if x * y != num_elements:\n",
        "      continue\n",
        "    if y > x:\n",
        "      continue\n",
        "    diff = x - y\n",
        "    if diff < best_diff:\n",
        "      best_diff = diff\n",
        "      result = (x, y)\n",
        "  return result\n",
        "\n",
        "def get_grid_dims_squareish(num_elements):\n",
        "  y = int(math.sqrt(num_elements))\n",
        "  x = y\n",
        "  while (x * y) < num_elements:\n",
        "    x += 1\n",
        "  return x, y\n",
        "\n",
        "def make_image_grid(images, conversion_func, blanks_ok=True):\n",
        "  individual_images = [conversion_func(image[None, :]) for image in images]\n",
        "  _, _, image_h, image_w = images.shape\n",
        "  return make_image_grid_from_pil(individual_images, image_h, image_w, captions, blanks_ok)\n",
        "\n",
        "def make_image_grid_from_pil(individual_images, image_h, image_w, captions=None, blanks_ok=True):\n",
        "\n",
        "  image_count = len(individual_images)\n",
        "  assert(image_w == image_h) #todo\n",
        "\n",
        "  if blanks_ok:\n",
        "    num_cols, num_rows = get_grid_dims_squareish(image_count)\n",
        "  else:\n",
        "    num_cols, num_rows = get_grid_dims(image_count)\n",
        "\n",
        "  w = image_w * num_cols\n",
        "  h = image_h * num_rows\n",
        "  combo = Image.new('RGB', (w, h))\n",
        "  if captions != None:\n",
        "    drawing_context = ImageDraw.Draw(combo)\n",
        "\n",
        "  for sample_num in range(image_count):\n",
        "    col = sample_num % num_cols\n",
        "    row = sample_num // num_cols\n",
        "    x = col * image_w\n",
        "    y = row * image_h\n",
        "    image = individual_images[sample_num]\n",
        "    combo.paste(image, (x, y))\n",
        "    if captions != None:\n",
        "      draw_text(captions[sample_num], x+1, y, drawing_context, font)\n",
        "\n",
        "  return combo\n",
        "\n",
        "def load_list_file(list_file):\n",
        "  with io.open(list_file,'rt', encoding='utf-8') as f:\n",
        "\n",
        "    lines = [line.rstrip() for line in f]\n",
        "\n",
        "    #contents = f.read()\n",
        "    ##print(f'load_list_file {contents}')\n",
        "    ##lines = contents.decode(\"ISO-8859-1\").rstrip(\"\\n\")\n",
        "    #lines = contents.rstrip(\"\\n\")\n",
        "    #lines = lines.split(\"\\n\")\n",
        "  #print(len(lines))\n",
        "  return lines\n",
        "  \n",
        "display_counter = 0\n",
        "def display_image(image):\n",
        "  global display_counter\n",
        "  print(f'display_counter {display_counter}')\n",
        "  if display_counter > 50:\n",
        "    display_counter = 0\n",
        "    display.clear_output()\n",
        "    print(f'display.clear_output()')\n",
        "  display.display(image)\n",
        "  display_counter += 1  \n",
        "\n",
        "def load_captions_from_file_list(file_list, match_suffix=None, match_substr=None, sample_rate=1.0, num_to_take=None, randomize_seed=False):\n",
        "\n",
        "  if randomize_seed:\n",
        "    original_state = np.random.get_state()\n",
        "    np.random.seed(int(time.time()))\n",
        "\n",
        "  if match_suffix != None or match_substr != None:\n",
        "    t = []\n",
        "    for fn in tqdm(file_list):\n",
        "      if (match_suffix == None or fn.endswith(match_suffix)) and (match_substr == None or match_substr in fn):\n",
        "        t.append(fn)\n",
        "    file_list = t\n",
        "\n",
        "  if (sample_rate > 0 and sample_rate < 1) or num_to_take != None:\n",
        "    random_idx = (list(range(0, len(file_list))))\n",
        "    np.random.shuffle(random_idx)\n",
        "    if num_to_take != None:\n",
        "      #random_idx = sorted(random_idx[:int(num_to_take)])\n",
        "      random_idx = (random_idx[:int(num_to_take)])\n",
        "    else:\n",
        "      #random_idx = sorted(random_idx[:int(len(random_idx) * sample_rate)])\n",
        "      random_idx = (random_idx[:int(len(random_idx) * sample_rate)])\n",
        "    file_list = [file_list[i] for i in random_idx]\n",
        "\n",
        "  result = []\n",
        "  for f in tqdm(file_list):\n",
        "    try:\n",
        "      caption = load_list_file(f)[0]\n",
        "      caption = caption.replace('\\r', '')\n",
        "      caption = caption.replace('\\n', '')\n",
        "      result.append(caption)\n",
        "    except Exception as e:\n",
        "      print(f'load_captions_from_path: Error loading {f}\\n{e}')\n",
        "\n",
        "  if randomize_seed:\n",
        "    np.random.set_state(original_state)\n",
        "\n",
        "  return result\n",
        "\n",
        "def shuffle_list(the_list):\n",
        "  random_idx = (list(range(0, len(the_list))))\n",
        "  np.random.shuffle(random_idx)\n",
        "  return [the_list[i] for i in random_idx]\n",
        "\n",
        "import datetime\n",
        "ring_size = 10\n",
        "timing_ring = ring_size*[0]\n",
        "ring_idx = 0\n",
        "\n",
        "def print_timing_stats(num_done, num_total, start_time, cur_time, n=None, update_freq = None):\n",
        "  try:\n",
        "    if num_done <= 0:\n",
        "        return\n",
        "    if update_freq == None:\n",
        "        update_freq = num_total // 10000\n",
        "    if int(num_done) % int(update_freq) != 0:\n",
        "        return\n",
        "\n",
        "    global timing_ring\n",
        "    global ring_idx\n",
        "    timing_ring[ring_idx%ring_size] = cur_time\n",
        "    ring_idx += 1\n",
        "    num = min(ring_size, ring_idx)\n",
        "    dt = (max(timing_ring[:num]) - min(timing_ring[:num])) / (num * update_freq)\n",
        "\n",
        "    #dt = cur_time - start_time\n",
        "    rate = dt# / num_done\n",
        "    eta = rate * (num_total - num_done)\n",
        "    formatted_eta = str(datetime.timedelta(seconds = eta))\n",
        "    if n != None:\n",
        "      n = f'n: {n}'\n",
        "    else:\n",
        "      n = ''\n",
        "    print(f'{num_done}/{num_total} ({(100.0*num_done/num_total):.2f}%) ETA: {formatted_eta} it/sec: {(1/rate):.2f} {n}')\n",
        "  except:\n",
        "    pass\n",
        "  \n",
        "def process_prompt(use_subjects, s, use_prompts, p, use_hard_prompt, hp):\n",
        "  result = ''\n",
        "  need_comma = False\n",
        "  if use_subjects:\n",
        "    result += s\n",
        "    need_comma = True\n",
        "  if use_hard_prompt:\n",
        "    if need_comma:\n",
        "      result += ', '\n",
        "    result += hp\n",
        "    need_comma = True\n",
        "  if use_prompts:\n",
        "    if need_comma:\n",
        "      result += ', '\n",
        "    result += p\n",
        "  return result\n",
        "\n",
        "class PromptFilename:\n",
        "  def __init__(self):\n",
        "    self.prompt_list = []\n",
        "    self.hash_list = []\n",
        "    pass\n",
        "\n",
        "  def add_prompt(self, prompt):\n",
        "    self.prompt_list.append(clean_filename(prompt))\n",
        "    self.hash_list.append(hash(prompt))\n",
        "\n",
        "  def get_string(self, max_len):\n",
        "    result = ''\n",
        "    for hash in self.hash_list:\n",
        "      result = f'{result}-ph`{hash:X}`'\n",
        "    #print(f'len(result) {len(result)}')\n",
        "    remaining_chars = max_len - len(result)\n",
        "    #print(f'remaining_chars {remaining_chars}')\n",
        "    chars_per_prompt = max(remaining_chars // len(self.prompt_list), 0)\n",
        "    #print(f'chars_per_prompt {chars_per_prompt}')\n",
        "    for prompt in self.prompt_list:\n",
        "      result = f'{result}+{prompt[:chars_per_prompt]}'\n",
        "    return result\n",
        "\n",
        "class CFGDenoiser(nn.Module):\n",
        "    def __init__(self, model):\n",
        "        super().__init__()\n",
        "        self.inner_model = model\n",
        "\n",
        "    def forward(self, x, sigma, uncond, cond, cond_scale):\n",
        "        x_in = torch.cat([x] * 2)\n",
        "        sigma_in = torch.cat([sigma] * 2)\n",
        "        cond_in = torch.cat([uncond, cond])\n",
        "        uncond, cond = self.inner_model(x_in, sigma_in, cond=cond_in).chunk(2)\n",
        "        return uncond + (cond - uncond) * cond_scale\n",
        "\n",
        "\n",
        "class KDiffusionSampler:\n",
        "    def __init__(self, m, sampler):\n",
        "        self.model = m\n",
        "        self.model_wrap = K.external.CompVisDenoiser(m)\n",
        "        self.schedule = sampler\n",
        "\n",
        "    def sample(self, S, conditioning, batch_size, shape, verbose, unconditional_guidance_scale, unconditional_conditioning, eta, x_T):\n",
        "        sigmas = self.model_wrap.get_sigmas(S)\n",
        "        x = x_T * sigmas[0]\n",
        "        model_wrap_cfg = CFGDenoiser(self.model_wrap)\n",
        "\n",
        "        samples_ddim = K.sampling.__dict__[f'sample_{self.schedule}'](model_wrap_cfg, x, sigmas, extra_args={'cond': conditioning, 'uncond': unconditional_conditioning, 'cond_scale': unconditional_guidance_scale}, disable=False)\n",
        "\n",
        "        return samples_ddim, None\n",
        "\n",
        "#https://discuss.pytorch.org/t/help-regarding-slerp-function-for-generative-model-sampling/32475/3\n",
        "def slerp(val, low, high):\n",
        "    low_norm = low/torch.norm(low, dim=1, keepdim=True)\n",
        "    high_norm = high/torch.norm(high, dim=1, keepdim=True)\n",
        "    omega = torch.acos((low_norm*high_norm).sum(1))\n",
        "    so = torch.sin(omega)\n",
        "    res = (torch.sin((1.0-val)*omega)/so).unsqueeze(1)*low + (torch.sin(val*omega)/so).unsqueeze(1) * high\n",
        "    return res        "
      ],
      "metadata": {
        "id": "0TTt7ghMA-Ib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Technical Settings\n",
        "See Python comments for variable descriptions. Many are unchanged from the original script.\n",
        "This section is for the boring settings you're not likely to change much"
      ],
      "metadata": {
        "id": "au7xrg1E50_F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#quality level for saved JPGs\n",
        "jpg_quality = 99 #@param {type:\"integer\"}\n",
        "\n",
        "#do not save a grid, only individual samples. Helpful when evaluating lots of samples\"\n",
        "skip_grid = False #@param {type:\"boolean\"}\n",
        "#do not save individual samples. For speed measurements.\"\n",
        "skip_save = False #@param {type:\"boolean\"}\n",
        "#ddim eta (eta=0.0 corresponds to deterministic sampling\"\n",
        "ddim_eta = 0.0 #@param {type:\"number\"}\n",
        "#sample this many times\"\n",
        "n_iter = 1 #@param {type:\"integer\"}\n",
        "#image height, in pixel space\"\n",
        "H = 512 #@param {type:\"integer\"}\n",
        "#image width, in pixel space\"\n",
        "W = 512 #@param {type:\"integer\"}\n",
        "#latent channels\"\n",
        "C = 4 #@param {type:\"integer\"}\n",
        "#downsampling factor\"\n",
        "f = 8 #@param {type:\"integer\"}\n",
        "#evaluate at this precision\"\n",
        "precision = \"autocast\" #@param {type:\"string\"}\n",
        "#      choices=[\"full\", \"autocast\"],\n",
        "\n",
        "#for grid layout, should blank cells be allowed?\n",
        "#True tries to make the grid as square as possible\n",
        "#False only uses grid dimensions that exactly multiply to the number of samples\n",
        "grid_mode_blanks_ok = False #@param {type:\"boolean\"}\n",
        "\n",
        "precision_scope = autocast if precision==\"autocast\" else nullcontext\n"
      ],
      "metadata": {
        "id": "FhfGDi-451mr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Settings that require model reload"
      ],
      "metadata": {
        "id": "WOfarod1QXij"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "force_model_reload = False #@param {type:\"boolean\"}\n",
        "\n",
        "if not 'model_loaded' in locals() or force_model_reload:\n",
        "  #path to config which constructs model\"\n",
        "  model_config_path = \"configs/stable-diffusion/v1-inference.yaml\" #@param {type:\"string\"}\n",
        "  #path to checkpoint of model\"\n",
        "  if model_path_from_gdrive != None:\n",
        "    ckpt = model_path_from_gdrive\n",
        "  else:\n",
        "    ckpt = r'models/ldm/stable-diffusion-v1/sd-v1-4.ckpt' #@param {type:\"raw\"}\n",
        "\n",
        "  config = OmegaConf.load(f\"{model_config_path}\")\n",
        "  model = load_model_from_config(config, f\"{ckpt}\")\n",
        "\n",
        "  device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "  model = model.to(device)\n",
        "\n",
        "  model_loaded = True"
      ],
      "metadata": {
        "id": "B0gGtsVvQaka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompts and prompt settings"
      ],
      "metadata": {
        "id": "mUN8la9h4O4X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#the prompts to render\n",
        "import random\n",
        "\n",
        "#if you have a file full of prompts, you can load it here\n",
        "#there's a simple prompt generator at the end of the notebook\n",
        "use_files = False #@param {type: \"boolean\"}\n",
        "repeats_per_prompt = 1 #@param {type: \"integer\"}\n",
        "\n",
        "use_subjects = False #@param {type: \"boolean\"}\n",
        "randomize_subjects = False #@param {type: \"boolean\"}\n",
        "subject_file_path = r'subjects.txt' #@param {type: \"raw\"}\n",
        "use_prompts = False #@param {type: \"boolean\"}\n",
        "randomize_prompts = False #@param {type: \"boolean\"}\n",
        "prompt_file_path = r'prompts.txt' #@param {type: \"raw\"}\n",
        "\n",
        "use_hard_prompt = False #@param {type: \"boolean\"}\n",
        "hard_prompt = \"something about a lighthouse playing a guitar, I think\"  #@param {type: \"string\"}\n",
        "\n",
        "prompot_combo_mode = \"one_each\" #@param [\"all\", \"one_each\"]\n",
        "\n",
        "subjects = ['']\n",
        "prompts = ['']\n",
        "\n",
        "if use_files:\n",
        "  if use_subjects:\n",
        "    subjects = load_list_file(subject_file_path)\n",
        "  if use_prompts:\n",
        "    prompts = load_list_file(prompt_file_path)\n",
        "\n",
        "  if randomize_subjects:\n",
        "    data = shuffle_list(subjects)\n",
        "\n",
        "  if randomize_prompts:\n",
        "    data = shuffle_list(data)\n",
        "\n",
        "  data = []\n",
        "  if prompot_combo_mode == \"all\":\n",
        "    for s in subjects:\n",
        "      for p in prompts:\n",
        "        p = process_prompt(use_subjects, s, use_prompts, p, use_hard_prompt, hard_prompt)\n",
        "        for r in range(repeats_per_prompt):\n",
        "          data.append(p)\n",
        "  if prompot_combo_mode == \"one_each\":\n",
        "    num_items = max(len(subjects), len(prompts))\n",
        "    for i in range(num_items):\n",
        "      s = subjects[i%len(subjects)]\n",
        "      p = prompts[i%len(prompts)]\n",
        "      p = process_prompt(use_subjects, s, use_prompts, p, use_hard_prompt, hard_prompt)\n",
        "      for r in range(repeats_per_prompt):      \n",
        "        data.append(p)\n",
        "  \n",
        "  print('\\n'.join(data[:100]))\n",
        "  print(len(data))\n",
        "\n",
        "else:\n",
        "  data = [\n",
        "    \"painting of a virus monster playing guitar\",\n",
        "    \"A beautiful painting of a singular lighthouse, shining its light across a tumultuous sea of blood by greg rutkowski and thomas kinkade, Trending on artstation.\",\n",
        "    \"In the foreground, happy chocolate beagles are playing in a field of wildflowers on a clear day, in the distance you can see mountains and a forest.\",\n",
        "    \"A sentient android made of wood, by Ivan shishkin and beeple, trending on artstation\",\n",
        "    \"The cybernetic humanoid stood in the haunted forest like an ancient statue, pen and colorful ink gouache painting by Hiroshi Yoshida, Dan Mumford, Artstation, Behance, atmospheric environment art\",\n",
        "    \"My mother told me a fairytale, about a whimsical fairy land found in an ancient children's storybook, comic illustration by Larry Elmore, Artstation, zbrushcentral\",\n",
        "    #\n",
        "    \"So glad I got up for this -15F sunrise. There's only a handful of days in the year where the morning light lines up perfectly with the hole in Hollow Rock. Grand Portage, MN. \",\n",
        "    \"For a 15-minute period during my flight back to Canada yesterday, there were no clouds blocking the view over Greenland's glaciers and icebergs \",\n",
        "    \"Woke up at 5AM to catch the tulips with morning mist, the Netherlands (OC)\",\n",
        "    \"Eclipse Phases over Brasstown Bald, Georgia \",\n",
        "    \"You might have seen it before, but here's that one place in Indonesia with a volcano behind waterfalls. \",\n",
        "    \"One of my scariest moments as a photographer- what you dont see here is the 100m drop in front of me and the gale force wind from behind. Two minutes of light and then it was dark again. Faroe islands \",\n",
        "    #  \n",
        "    \"This Neo-Tokyo is now a wasteland after the cybernetic robots attacked, pen and colorful ink gouache painting by Hiroshi Yoshida, Dan Mumford, Artstation, Behance, atmospheric environment art\",\n",
        "    \"Faceless Phantoms surrounded her asking where she was heading, she's lost walking down a desolate road, pen and colorful ink gouache painting by Hiroshi Yoshida, Naoko Takeuchi, Artstation, Behance, magical fantasy art\",\n",
        "    \"A vast magic shop filled with potion bottles and oddities, 3D illustration by Greg Rutkowski and Gediminas Pranckevicius, Artstation, zbrushcentral, 3D shading, magical realism\",\n",
        "    \"Bacon double cheeseburger and fries, tarot card by Alphonse Mucha\",\n",
        "    \"A tropical sea port at night, mountains and a small city in the background artstation, cgsociety, matte painting, in watercolor\",\n",
        "    \"Landscape oil on canvas by Bob Ross\",\n",
        "  ]\n",
        "\n",
        "#if True, each sample in the batch gets prefixed with the corresponding entry in this list\n",
        "use_slot_modifiers = False #@param {type:\"boolean\"}\n",
        "\n",
        "#various slot modifier ideas\n",
        "#These examples have 6 entries because that's the batch size I'm using. If you use bigger batches and want to use this feature, you'll need longer lists.\n",
        "slot_modifiers = ['red ', 'orange ', 'yellow ', 'green ', 'blue ', 'purple ']\n",
        "#slot_modifiers = ['majestic', 'made of legos', 'fiery', 'firendly', 'stuffed animal', 'made of cake']\n",
        "#slot_modifiers = ['Superman in ', 'Batman in ', 'Wonder Woman in ', 'The Flash in ', 'Aquaman in ', 'Harley Quinn in ']\n",
        "#slot_modifiers = ['Captain America in ', 'Iron Man in ', 'Black Widow in ', 'Spider-Man in ', 'The Hulk in ', 'Thor in ']\n",
        "#slot_modifiers = ['art nouveau ', 'comic book ', 'stained glass ', 'a painting by Mary Jane Ansell of ', 'claymation ', 'a marble sculpture of ']\n",
        "#slot_modifiers = ['award winning', 'top rated', \"world's best\", 'incredibly detailed', 'hyperrealistic', 'artstation']\n"
      ],
      "metadata": {
        "id": "72rh0wLh4HyJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# This is where the magic happens\n",
        "\n",
        "Frequently-used settings go here"
      ],
      "metadata": {
        "id": "PBga2cPYnLGS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#dirs to write results to\n",
        "outdir = r\"gallery\" #@param {type:\"raw\"}\n",
        "ckpt_name = os.path.split(ckpt)[1]\n",
        "ckpt_name = os.path.splitext(ckpt_name)[0]\n",
        "outdir = os.path.join(outdir, ckpt_name)\n",
        "sample_path = os.path.join(outdir, \"samples\") #@param {type:\"raw\"}\n",
        "os.makedirs(sample_path, exist_ok=True)\n",
        "grid_path = os.path.join(outdir, \"grids\") #@param {type:\"raw\"}\n",
        "#debug images show the prompts and blend values\n",
        "grid_d_path = os.path.join(outdir, \"grids_debug\") #@param {type:\"raw\"}\n",
        "os.makedirs(grid_path, exist_ok=True)\n",
        "os.makedirs(grid_d_path, exist_ok=True)\n",
        "#how many samples to produce for each given prompt. A.k.a. batch size\"\n",
        "#A 3090 can handle a maximum of 6 at 512x512\n",
        "batch_size = 3 #@param {type:\"integer\"}\n",
        "\n",
        "#the seed (for reproducible sampling)\"\n",
        "seed = int(time.time()) #@param {type:\"raw\"}\n",
        "#useful if you're tyring to evaluate a large number of seeds\n",
        "increment_seed_per_iter = False #@param {type:\"boolean\"}\n",
        "\n",
        "# 1 step means there's no interpolation\n",
        "num_interpolation_steps = 4 #@param {type:\"integer\"}\n",
        "#if True, new start codes are gerated per batch. If false, start codes are only generated once at the beginning of the run.\n",
        "new_start_code_per_batch = True #@param {type:\"boolean\"}\n",
        "#if True, each sample in the batch gets a different start code.  If False, each sample in the batch gets the same start code.\n",
        "new_start_code_per_sample = True #@param {type:\"boolean\"}\n",
        "\n",
        "#batch_shift: each prompt blends with the one in the same slot of the next batch\n",
        "#right_shift: each prompt blends with the one to its right (or it wraps around to the first one, in the case of the last sample in the batch)\n",
        "prompt_blend_mode = 'batch_shift' #@param ['batch_shift', 'right_shift']\n",
        "\n",
        "# The path taken through conditioning space when interpolating. Can be linear or spherical. Personally I think linear looks better, probably because its velocity is lower.\n",
        "prompt_blend_type = 'linear' #@param ['linear', 'spherical']\n",
        "\n",
        "#if 'per_sample', each sample in the branch uses a different prompt. If 'per_batch', each sample in the batch uses the same prompt.\n",
        "prompt_mode = 'per_sample' #@param ['per_batch', 'per_sample']\n",
        "\n",
        "#number of ddim sampling steps\"\n",
        "ddim_steps_min = 20 #@param {type:\"integer\"}\n",
        "ddim_steps_max = 20 #@param {type:\"integer\"}\n",
        "\n",
        "#unconditional guidance scale: eps = eps(x, empty) + scale * (eps(x, cond) - eps(x, empty))\"\n",
        "scale = 7.5 #@param {type:\"string\"}\n",
        "\n",
        "# you can enable more than one sampler\n",
        "sampler_names = []\n",
        "use_PLMS = False #@param {type:'boolean'}\n",
        "if use_PLMS: sampler_names.append('PLMS')\n",
        "use_DDIM = False #@param {type:'boolean'}\n",
        "if use_DDIM: sampler_names.append('DDIM')\n",
        "use_k_dpm_2_a = False #@param {type:'boolean'}\n",
        "if use_k_dpm_2_a: sampler_names.append('k_dpm_2_a')\n",
        "use_k_dpm_2 = False #@param {type:'boolean'}\n",
        "if use_k_dpm_2: sampler_names.append('k_dpm_2')\n",
        "use_k_euler_a = False #@param {type:'boolean'}\n",
        "if use_k_euler_a: sampler_names.append('k_euler_a')\n",
        "use_k_euler = False #@param {type:'boolean'}\n",
        "if use_k_euler: sampler_names.append('k_euler')\n",
        "use_k_heun = False #@param {type:'boolean'}\n",
        "if use_k_heun: sampler_names.append('k_heun')\n",
        "use_k_lms = True #@param {type:'boolean'}\n",
        "if use_k_lms: sampler_names.append('k_lms')\n",
        "\n",
        "for sampler_name in sampler_names:\n",
        "  if sampler_name == 'PLMS':\n",
        "      sampler = PLMSSampler(model)\n",
        "  elif sampler_name == 'DDIM':\n",
        "      sampler = DDIMSampler(model)\n",
        "  elif sampler_name == 'k_dpm_2_a':\n",
        "      sampler = KDiffusionSampler(model,'dpm_2_ancestral')\n",
        "  elif sampler_name == 'k_dpm_2':\n",
        "      sampler = KDiffusionSampler(model,'dpm_2')\n",
        "  elif sampler_name == 'k_euler_a':\n",
        "      sampler = KDiffusionSampler(model,'euler_ancestral')\n",
        "  elif sampler_name == 'k_euler':\n",
        "      sampler = KDiffusionSampler(model,'euler')\n",
        "  elif sampler_name == 'k_heun':\n",
        "      sampler = KDiffusionSampler(model,'heun')\n",
        "  elif sampler_name == 'k_lms':\n",
        "      sampler = KDiffusionSampler(model,'lms')\n",
        "  else:\n",
        "      raise Exception(\"Unknown sampler: \" + sampler_name)    \n",
        "\n",
        "  start_codes = []\n",
        "  seed_everything(seed)\n",
        "  start_codes.append(make_start_code(batch_size, not new_start_code_per_sample, C, H, W, f, device))\n",
        "  start_codes.append(make_start_code(batch_size, not new_start_code_per_sample, C, H, W, f, device))\n",
        "  frame = 0\n",
        "\n",
        "  original_start_code = start_codes[0]\n",
        "  #print(f'original_start_code {original_start_code}')\n",
        "\n",
        "  with torch.no_grad():\n",
        "      with precision_scope(\"cuda\"):\n",
        "          with model.ema_scope():\n",
        "              tic = time.time()\n",
        "              all_samples = list()\n",
        "              for n in trange(n_iter, desc=\"Sampling\"):\n",
        "                  if n > 0:\n",
        "                    if increment_seed_per_iter:\n",
        "                      #seed = time.time()\n",
        "                      seed += 1\n",
        "                      print(f'new seed: {seed}')\n",
        "                      seed_everything(seed)\n",
        "                      start_codes[0] = make_start_code(batch_size, not new_start_code_per_sample, C, H, W, f, device)\n",
        "                      start_codes[1] = make_start_code(batch_size, not new_start_code_per_sample, C, H, W, f, device)\n",
        "\n",
        "                  if prompt_mode == 'per_sample':\n",
        "                    num_batches = len(data) // batch_size\n",
        "                  elif prompt_mode == 'per_batch':\n",
        "                    num_batches = len(data)\n",
        "                  num_batches *= len(sampler_names)\n",
        "                  interp_batch = 0\n",
        "                  for batch_num in tqdm(range(num_batches), desc=\"data\"):\n",
        "                    if new_start_code_per_batch:\n",
        "                      start_codes[0] = start_codes[1]\n",
        "                      print(f'batch_num {batch_num}')\n",
        "                      if batch_num == num_batches - 1:\n",
        "                        print('looping')\n",
        "                        start_codes[1] = original_start_code\n",
        "                        #print(f'original_start_code {original_start_code}')\n",
        "                      else:\n",
        "                        start_codes[1] = make_start_code(batch_size, not new_start_code_per_sample, C, H, W, f, device)\n",
        "\n",
        "                    #todo: add support for more parameter sweeps\n",
        "                    #for scale in np.arange(4, 9.01, 1.0).tolist():\n",
        "                    #for scale in np.arange(0, 100.01, 1.0).tolist():\n",
        "                    #for batch_size in range(1, 1000):\n",
        "                    #for ddim_steps in range(1, 1001):\n",
        "                    for ddim_steps in range(ddim_steps_min, ddim_steps_max+1):\n",
        "                      if 1000 % ddim_steps != 0:\n",
        "                        continue\n",
        "                      num_interp_batches = num_interpolation_steps * num_batches\n",
        "                      start_time = time.time()\n",
        "                      for prompt_interp in tqdm(np.arange(0, 1.0, (1.0/float(num_interpolation_steps))).tolist()):\n",
        "                        sampling_start_time = time.time()                    \n",
        "                        print_timing_stats(interp_batch, num_interp_batches, start_time, time.time(), n=None, update_freq = 1)\n",
        "                        interp_batch += 1\n",
        "                        prompt_interp_inv = 1.0 - prompt_interp\n",
        "\n",
        "                        if new_start_code_per_batch:\n",
        "                          #start_code = start_codes[0][:batch_size, :] * prompt_interp_inv + start_codes[1][:batch_size, :] * prompt_interp\n",
        "                          #lerp doesn't work well in this space, use slerp instead\n",
        "                          start_code = slerp(prompt_interp, start_codes[0][:batch_size, :], start_codes[1][:batch_size, :])\n",
        "                        else:\n",
        "                          start_code = start_codes[0][:batch_size, :]\n",
        "\n",
        "\n",
        "                        uc = None\n",
        "                        if scale != 1.0:\n",
        "                            uc = model.get_learned_conditioning(batch_size * [\"\"])\n",
        "                        #if isinstance(prompt, tuple):\n",
        "                        #    prompts = list(prompt)\n",
        "                        #else:\n",
        "\n",
        "                        if prompt_mode == 'per_sample':\n",
        "                          prompts = data[batch_num*batch_size : (batch_num+1)*batch_size]\n",
        "                        elif prompt_mode == 'per_batch':\n",
        "                          prompts = [data[batch_num%len(data)]] * batch_size\n",
        "                        \n",
        "                        #print(f'\\n'.join(prompts))\n",
        "                        if use_slot_modifiers:\n",
        "                          for b in range(batch_size):\n",
        "                            prompts[b] = slot_modifiers[b] + ', ' +prompts[b]\n",
        "                        c = model.get_learned_conditioning(prompts)\n",
        "\n",
        "                        if prompt_blend_mode != None:\n",
        "                          if prompt_blend_mode =='right_shift':\n",
        "                            blended_c = torch.zeros_like(c)\n",
        "                            next_prompts = []\n",
        "                            for prompt_num in range(batch_size):\n",
        "                              if prompt_blend_type == 'linear':\n",
        "                                blended_c[prompt_num] = c[prompt_num] * prompt_interp_inv + c[(prompt_num+1) % batch_size] * prompt_interp\n",
        "                              elif prompt_blend_type == 'spherical':\n",
        "                                blended_c[prompt_num] = slerp(prompt_interp, c[prompt_num], c[(prompt_num+1) % batch_size])\n",
        "                              next_prompts.append(prompts[(prompt_num+1) % batch_size])\n",
        "                            c = blended_c\n",
        "                          elif prompt_blend_mode =='batch_shift':\n",
        "                            \n",
        "                            if prompt_mode == 'per_sample':\n",
        "                              next_begin = (batch_num+1)*batch_size\n",
        "                              next_end   = (batch_num+2)*batch_size\n",
        "                              next_prompts = []\n",
        "                              for i in range(next_begin, next_end):\n",
        "                                i = i % len(data)\n",
        "                                next_prompts.append(data[i])\n",
        "                            elif prompt_mode == 'per_batch':\n",
        "                              next_prompts = [data[(batch_num+1)%len(data)]] * batch_size\n",
        "\n",
        "                            if use_slot_modifiers:\n",
        "                              for b in range(batch_size):\n",
        "                                next_prompts[b] = slot_modifiers[b] + ', ' +next_prompts[b]\n",
        "                            next_c = model.get_learned_conditioning(next_prompts)\n",
        "                            \n",
        "\n",
        "                            if prompt_blend_type == 'linear':\n",
        "                              c = c * prompt_interp_inv + next_c * prompt_interp\n",
        "                            elif prompt_blend_type == 'spherical':\n",
        "                              c = slerp(prompt_interp, c, next_c)\n",
        "\n",
        "\n",
        "                        shape = [C, H // f, W // f]\n",
        "                        try:\n",
        "                          samples_ddim, _ = sampler.sample(S=ddim_steps,\n",
        "                                                            conditioning=c,\n",
        "                                                            batch_size=batch_size,\n",
        "                                                            shape=shape,\n",
        "                                                            verbose=False,\n",
        "                                                            unconditional_guidance_scale=scale,\n",
        "                                                            unconditional_conditioning=uc,\n",
        "                                                            eta=ddim_eta,\n",
        "                                                            x_T=start_code)\n",
        "                        except Exception as e:\n",
        "                          print(f'Exception in sampler.sample\\n{e}')\n",
        "                          continue\n",
        "\n",
        "\n",
        "                        x_samples_ddim = model.decode_first_stage(samples_ddim)\n",
        "                        x_samples_ddim = torch.clamp((x_samples_ddim + 1.0) / 2.0, min=0.0, max=1.0)\n",
        "\n",
        "                        all_images = []\n",
        "\n",
        "                        if not skip_save:\n",
        "                            for x_sample in x_samples_ddim:\n",
        "                                x_sample = 255. * rearrange(x_sample.cpu().numpy(), 'c h w -> h w c')\n",
        "                                all_images.append(Image.fromarray(x_sample.astype(np.uint8)))\n",
        "\n",
        "                            \n",
        "                            grid_name = PromptFilename()\n",
        "                            for prompt_string in prompts:\n",
        "                              grid_name.add_prompt(prompt_string)\n",
        "                              #grid_name.append(clean_filename(prompt_string)[:20])\n",
        "                            cleaned_name = grid_name.get_string(max_len=150)\n",
        "                            for suffix in range(0, 100000): #todo, cleanup\n",
        "                              #gs means guidance scale\n",
        "                              output_filepath = os.path.join(grid_path, f\"{frame:06d}{cleaned_name}-gs_{scale}-{sampler_name}{ddim_steps}-pi{prompt_interp:.04f}-s{seed}-{suffix:04d}.jpg\")\n",
        "                              grid_prompt_filepath = os.path.join(grid_d_path, f'{cleaned_name}.txt')\n",
        "                              if not os.path.isfile(output_filepath):\n",
        "                                break\n",
        "\n",
        "                            if prompt_blend_mode != None and prompt_interp_inv != 1.0:\n",
        "                              d_captions = ['\\n'.join([f'{prompt_interp_inv:.02f} {prompts[i]}', f'{prompt_interp:.02f} {next_prompts[i]}']) for i in range(batch_size)]\n",
        "                            else:\n",
        "                              d_captions = prompts\n",
        "\n",
        "                            if not skip_grid and batch_size > 1:\n",
        "                              image_grid = make_image_grid_from_pil(all_images, H, W, blanks_ok=True)\n",
        "                              image_grid.save(output_filepath, quality=jpg_quality, optimize=True)\n",
        "                              image_grid = make_image_grid_from_pil(all_images, H, W, captions=d_captions, blanks_ok=True)\n",
        "                              display_image(image_grid)\n",
        "                              grid_filename = output_filepath.replace(grid_path, grid_d_path)\n",
        "                              image_grid.save(grid_filename, quality=jpg_quality, optimize=True)\n",
        "                              with io.open(grid_prompt_filepath, 'wt', encoding='utf-8') as caption_file:\n",
        "                                caption_file.write('\\n\\n'.join(d_captions))\n",
        "                              \n",
        "                              #print(f'saved {output_filepath}')\n",
        "                            save_captions = []\n",
        "                            for image_num, image in enumerate(all_images):\n",
        "                              image_name = PromptFilename()\n",
        "                              image_name.add_prompt(prompts[image_num])\n",
        "                              #cleaned_name = clean_filename(prompts[image_num])\n",
        "                              #cleaned_name = f'{cleaned_name[:100]}-ph{hash(prompts[image_num])}'\n",
        "                              save_caption = [prompts[image_num]]\n",
        "                              if prompt_blend_mode != None and prompt_interp_inv != 1.0:\n",
        "                                #cleaned_name += '+' + clean_filename(next_prompts[image_num])\n",
        "                                image_name.add_prompt(next_prompts[image_num])\n",
        "                                save_caption.append(next_prompts[image_num])\n",
        "                              cleaned_name = image_name.get_string(max_len=150)\n",
        "                              save_captions.append(save_caption)\n",
        "                              for suffix in range(0, 100000): #todo, cleanup\n",
        "                                image_path = os.path.join(sample_path, f\"{image_num:02d}-{frame:06d}{cleaned_name}-gs_{scale}-{sampler_name}{ddim_steps}-pi{prompt_interp:.04f}-s{seed}-{suffix:04d}.jpg\")\n",
        "                                image_prompt_path = os.path.join(sample_path, f\"{cleaned_name}.txt\")\n",
        "                                if not os.path.isfile(image_path):\n",
        "                                  break\n",
        "                              if skip_grid:\n",
        "                                display_image(image)\n",
        "                              image.save(image_path, quality=jpg_quality, optimize=True)\n",
        "                              with io.open(image_prompt_path, 'wt', encoding='utf-8') as caption_file:\n",
        "                                caption_file.write(d_captions[image_num])\n",
        "                              if batch_size == 1:\n",
        "                                display_image(image)\n",
        "                              #print(f'saved {image_path}')\n",
        "                              \n",
        "                          \n",
        "                            sampling_dtime =  time.time() - sampling_start_time\n",
        "                            print(f'Generated {batch_size} samples in {sampling_dtime}, {batch_size / sampling_dtime} samples per second')\n",
        "                            try:\n",
        "                              with io.open('gen_time_log.csv', 'at') as gen_time_log:\n",
        "                                log_line = f'{batch_size},{sampling_dtime},{batch_size / sampling_dtime},{scale},{sampler_name},{ddim_steps},{seed}\\n'\n",
        "                                gen_time_log.write(log_line)\n",
        "                            except Exception as e:\n",
        "                              print(f'Exception writing gen_time_log.csv:\\n{e}') \n",
        "                        frame += 1\n",
        "\n",
        "              toc = time.time()\n",
        "\n",
        "print(f\"Your samples are ready and waiting for you here: \\n{outdir} \\n\"\n",
        "      f\" \\nEnjoy.\")"
      ],
      "metadata": {
        "id": "pcbYHmqH4Ijr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Video (todo)"
      ],
      "metadata": {
        "id": "HfhicWS6qoEN"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XqNXS9nWtiHU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompt combiner\n",
        "Not necessary for the above, just a little tool to help make combinations of prompts and subjects."
      ],
      "metadata": {
        "id": "aHMWC7XXtimS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "subjects = [\n",
        "\"Beagle Puppy\",\n",
        "\"Maine Coon Cat\",\n",
        "\"Colorful macaw\",\n",
        "]\n",
        "\n",
        "generated_prompts = []\n",
        "for name in subjects:\n",
        "  scenes = [\n",
        "  f\"An incredibly beautiful richly colored portrait illustration of the Tarot {name} in the style of stained glass by Alphonse Mucha as featured on Artstation\",\n",
        "  f\"beautiful {name} portrait photo photograph face, artstation\",\n",
        "  f\"Conceptual Portrait of {name}, 35mm Portrait, vivid gouache and oil matte character portrait by Josephine Wall and Kelly McKernan, inspired by Disney, Artstation, CGsociety, 3d shading, Character concept art, #oc, character development\",\n",
        "  f\"extremely detailed depiction of {name} in ominous timeless space casting a spell that emits a colorful {name} by artstation warcraft\",\n",
        "  f\"Portrait of Cybernetic Cyberpunk {name}, vivid character portrait by Kelly McKernan and Skeeva, Artstation, CGsociety, 3d shading, Character concept art, #oc, character development\",\n",
        "  f\"{name} stands in front of three large mirrors isolated staring at the reflection, the reflection in the mirrors are of a {name}, each mirror depicting a different {name}, vivid gouache and oil 3D illustration by Greg Rutkowski and Mark Ryden, Artstation, zbrushcentral, cel shading, magic realism.\",\n",
        "  ]  \n",
        "  for scene in scenes:\n",
        "    print(scene)\n",
        "    generated_prompts.append(scene)\n",
        "\n",
        "  save_generated_prompt_file = False #@param {type: \"boolean\"}\n",
        "  if save_generated_prompt_file:\n",
        "    with open('prompts.txt', 'wt') as prompt_out_file:\n",
        "      prompt_out_file.write('\\n'.join(generated_prompts))\n",
        "\n",
        "    if save_generated_prompt_file:\n",
        "      data = generated_prompts\n"
      ],
      "metadata": {
        "id": "SpHihaTNqsOw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}